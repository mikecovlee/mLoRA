{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# m-LoRA: An Efficient \"Factory\" to Build Multiple LoRA Adapters\n",
    "[![](https://github.com/mikecovlee/mLoRA/actions/workflows/python-test.yml/badge.svg)](https://github.com/mikecovlee/mLoRA/actions/workflows/python-test.yml)\n",
    "[![](https://img.shields.io/github/stars/mikecovlee/mLoRA?logo=GitHub&style=flat)](https://github.com/mikecovlee/mLoRA/stargazers)\n",
    "[![](https://img.shields.io/github/v/release/mikecovlee/mLoRA?logo=Github)](https://github.com/mikecovlee/mLoRA/releases/latest)\n",
    "[![](https://img.shields.io/pypi/v/mlora?logo=pypi)](https://pypi.org/project/mlora/)\n",
    "[![](https://img.shields.io/docker/v/mikecovlee/mlora?logo=Docker&label=docker)](https://hub.docker.com/r/mikecovlee/mlora/tags)\n",
    "[![](https://img.shields.io/github/license/mikecovlee/mLoRA)](http://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "mLoRA (a.k.a Multi-LoRA Fine-Tune) is an open-source framework designed for efficient fine-tuning of multiple Large Language Models (LLMs) using LoRA and its variants. Key features of mLoRA include:\n",
    "\n",
    "- Concurrent fine-tuning of multiple LoRA adapters.\n",
    "\n",
    "- Shared base model among multiple LoRA adapters.\n",
    "\n",
    "- Support for multiple LoRA variant algorithms and various base models.\n",
    "\n",
    "- Exclusive Mo-LoRA (Mixture of LoRAs) optimization for MixLoRA and its variants.\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "This is a simple jupiter notebook for showcasing the basic process of fine-tuning TinyLLaMA with dummy data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone and install m-LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mlora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import mlora\n",
    "\n",
    "mlora.setup_logging(\"INFO\")\n",
    "\n",
    "base_model = \"TinyLlama/TinyLlama_v1.1\"\n",
    "\n",
    "model = mlora.LLMModel.from_pretrained(\n",
    "    base_model,\n",
    "    device=mlora.get_backend().default_device_name(),\n",
    "    load_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = mlora.Tokenizer(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a dummy LoRA adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = mlora.LoraConfig(\n",
    "    adapter_name=\"lora_0\",\n",
    "    lora_r_=32,\n",
    "    lora_alpha_=64,\n",
    "    lora_dropout_=0.05,\n",
    "    target_modules_={\"q_proj\": True, \"k_proj\": True, \"v_proj\": True, \"o_proj\": True},\n",
    ")\n",
    "\n",
    "model.init_adapter(lora_config)\n",
    "\n",
    "train_config = mlora.TrainConfig(\n",
    "    num_epochs=10,\n",
    "    batch_size=16,\n",
    "    micro_batch_size=8,\n",
    "    learning_rate=1e-4,\n",
    "    casual_train_data=\"TUDB-Labs/Dummy-mLoRA\",\n",
    ").init(lora_config)\n",
    "\n",
    "mlora.train(model=model, tokenizer=tokenizer, configs=[train_config])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the effectiveness of LoRA adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config = mlora.GenerateConfig(\n",
    "    adapter_name=\"lora_0\",\n",
    "    prompts=[\"Could you provide an introduction to m-LoRA?\"],\n",
    "    stop_token=\"\\n\",\n",
    ")\n",
    "\n",
    "output = mlora.generate(model=model, tokenizer=tokenizer, configs=[generate_config])\n",
    "\n",
    "print(output[\"lora_0\"][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
