{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# m-LoRA: An Efficient \"Factory\" to Build Multiple LoRA Adapters\n",
       "[![](https://github.com/scukdde-llm/mlora/actions/workflows/python-test.yml/badge.svg)](https://github.com/scukdde-llm/mlora/actions/workflows/python-test.yml)\n",
       "[![](https://img.shields.io/github/stars/scukdde-llm/mlora?logo=GitHub&style=flat)](https://github.com/scukdde-llm/mlora/stargazers)\n",
       "[![](https://img.shields.io/github/v/release/scukdde-llm/mlora?logo=Github)](https://github.com/scukdde-llm/mlora/releases/latest)\n",
       "[![](https://img.shields.io/pypi/v/mlora?logo=pypi)](https://pypi.org/project/mlora/)\n",
       "[![](https://img.shields.io/docker/v/mikecovlee/mlora?logo=Docker&label=docker)](https://hub.docker.com/r/mikecovlee/mlora/tags)\n",
       "[![](https://img.shields.io/github/license/scukdde-llm/mlora)](http://www.apache.org/licenses/LICENSE-2.0)\n",
       "\n",
       "mLoRA (a.k.a Multi-LoRA Fine-Tune) is an open-source framework designed for efficient fine-tuning of multiple Large Language Models (LLMs) using LoRA and its variants. Key features of mLoRA include:\n",
       "\n",
       "- Concurrent fine-tuning of multiple LoRA adapters.\n",
       "\n",
       "- Shared base model among multiple LoRA adapters.\n",
       "\n",
       "- Support for multiple LoRA variant algorithms and various base models.\n",
       "\n",
       "- Exclusive Mo-LoRA (Mixture of LoRAs) optimization for MixLoRA and its variants.\n",
       "\n",
       "## About this notebook\n",
       "\n",
       "This is a simple jupiter notebook for showcasing the basic process of fine-tuning TinyLLaMA with dummy data"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Clone and install m-LoRA"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "! git clone https://github.com/scukdde-llm/mLoRA\n",
       "%cd mLoRA\n",
       "! git pull\n",
       "! pip install ."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Loading the base model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import torch\n",
       "\n",
       "import mlora\n",
       "\n",
       "mlora.setup_logging(\"INFO\")\n",
       "\n",
       "base_model = \"TinyLlama/TinyLlama_v1.1\"\n",
       "\n",
       "model = mlora.LLMModel.from_pretrained(\n",
       "    base_model,\n",
       "    device=mlora.get_backend().default_device_name(),\n",
       "    load_dtype=torch.bfloat16,\n",
       ")\n",
       "tokenizer = mlora.Tokenizer(base_model)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Training a dummy LoRA adapter"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "lora_config = mlora.LoraConfig(\n",
       "    adapter_name=\"lora_0\",\n",
       "    lora_r_=32,\n",
       "    lora_alpha_=64,\n",
       "    lora_dropout_=0.05,\n",
       "    target_modules_={\"q_proj\": True, \"k_proj\": True, \"v_proj\": True, \"o_proj\": True},\n",
       ")\n",
       "\n",
       "model.init_adapter(lora_config)\n",
       "\n",
       "train_config = mlora.TrainConfig(\n",
       "    num_epochs=10,\n",
       "    batch_size=16,\n",
       "    micro_batch_size=8,\n",
       "    learning_rate=1e-4,\n",
       "    casual_data_path=\"data/dummy_data.json\",\n",
       "    casual_prompt_template=\"template/alpaca.json\",\n",
       ").init(lora_config)\n",
       "\n",
       "mlora.train(model=model, tokenizer=tokenizer, configs=[train_config])"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Validate the effectiveness of LoRA adapter"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "model.load_adapter(\"lora_0\")\n",
       "\n",
       "generate_config = mlora.GenerateConfig(\n",
       "    adapter_name=\"lora_0\",\n",
       "    prompt_template=\"template/alpaca.json\",\n",
       "    prompts=[\"What is m-LoRA?\"],\n",
       ")\n",
       "\n",
       "output = mlora.generate(model=model, tokenizer=tokenizer, configs=[generate_config], max_gen_len=64)\n",
       "\n",
       "print(output[\"lora_0\"][0])"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "mlora",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }