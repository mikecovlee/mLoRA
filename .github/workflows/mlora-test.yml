name: Test LLM Models

on: [push, pull_request]

jobs:
  test-llama:
    runs-on: self-hosted
    container:
      image: mikecovlee/mlora:0.2.1.dev1
      volumes:
        - /home/lab/models:/host_models/
      options: --gpus "device=1"
    steps:
      - uses: actions/checkout@v3
      - name: finetune lora
        run: |
          python launch.py gen --template lora --tasks ./data/dummy_data.json
          python launch.py train --base_model /host_models/TinyLlama-1.1B-intermediate-step-1431k-3T --attn_impl eager --quantize 8bit --dtype fp16
      - name: inference with lora
        run: |
          python generate.py --base_model /host_models/TinyLlama-1.1B-intermediate-step-1431k-3T --template "./template/alpaca.json" --lora_weights "./casual_0" --instruction "What is m-LoRA?" --max_seq_len 64

  test-gemma:
    runs-on: self-hosted
    container:
      image: mikecovlee/mlora:0.2.1.dev1
      volumes:
        - /home/lab/models:/host_models/
      options: --gpus "device=1"
    steps:
      - uses: actions/checkout@v3
      - name: finetune lora
        run: |
          python launch.py gen --template lora --tasks ./data/dummy_data.json
          python launch.py train --base_model /host_models/gemma-2b --attn_impl eager --quantize 8bit --dtype fp16
      - name: inference with lora
        run: |
          python generate.py --base_model /host_models/gemma-2b --template "./template/alpaca.json" --lora_weights "./casual_0" --instruction "What is m-LoRA?" --max_seq_len 64

  test-phi:
    runs-on: self-hosted
    container:
      image: mikecovlee/mlora:0.2.1.dev1
      volumes:
        - /home/lab/models:/host_models/
      options: --gpus "device=1"
    steps:
      - uses: actions/checkout@v3
      - name: finetune lora
        run: |
          python launch.py gen --template lora_phi --tasks ./data/dummy_data.json
          python launch.py train --base_model /host_models/phi-2 --attn_impl eager --quantize 8bit --dtype fp16
      - name: inference with lora
        run: |
          python generate.py --base_model /host_models/phi-2 --template "./template/alpaca.json" --lora_weights "./casual_0" --instruction "What is m-LoRA?" --max_seq_len 64
