name: Test LLM Models

on: [push, pull_request]

jobs:
  test-lora:
    runs-on: self-hosted
    container:
      image: mikecovlee/mlora:0.2.1.dev1
      volumes:
        - /home/lab/models:/host_models/
      options: --gpus 1
    steps:
      - uses: actions/checkout@v3
      - name: Install dependencies
        run: |
          pip3 install -r requirements.txt
          bash install_linux.sh
      - name: finetune lora
        run: |
          python launch.py gen --template lora --tasks ./data/dummy_data.json
          python launch.py train --base_model /host_models/TinyLlama-1.1B-intermediate-step-1431k-3T --attn_impl eager
      - name: inference with lora
        run: |
          python generate.py --base_model /host_models/TinyLlama-1.1B-intermediate-step-1431k-3T --template "./template/alpaca.json" --lora_weights "./casual_0" --instruction "What is m-LoRA?" --max_seq_len 64
