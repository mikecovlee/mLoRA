ChatGLMForConditionalGeneration(
    (transformer): ChatGLMModel(
        (embedding): Embedding(
            (word_embedding): Embedding(65024, 4096)
        )
        (rotary_pos_emb): RotaryEmbedding()
        (encoder): GLMTransformer(
            (layers): ModuleList(
                (0-27): 28 x GLMBlock(
                    (input_layernorm): RMSNorm()
                    (self_attention): SelfAttention(
                        (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)
                        (core_attention): CoreAttention(
                            (attention_dropout): Dropout(p=0.0, inplace=False)
                        )
                        (dense): Linear(in_features=4096, out_features=4096, bias=False)
                    )
                    (post_attention_layernorm): RMSNorm()
                    (mlp): MLP(
                        (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)
                        (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)
                    )
                )
            )
            (final_layernorm): RMSNorm()
        )
        (output_layer): Linear(in_features=4096, out_features=65024, bias=False)
    )
)

ChatGLMConfig {
    "_name_or_path": "/root/workspace/models/THUDM/chatg1m3-6b",
    "add_bias_linear": false,
    "add_qkv_bias": true,
    "apply_query_key_layer_scaling": true,
    "apply_residual_connection_post_layernorm": false,
    "architectures": [
        "ChatGLMModel"
    ],
    "attention_dropout: 0.0,
    "attention_softmax_in_fp32": true,
    "auto_map": {
        "AutoConfig": "configuration_chatglm.CHatGLMConfig",
        "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
        "AutoModelForCasualLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
        "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
        "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification",
    }
    "bias_dropout_fusion": true,
    "classifier_dropout": null,
    "eos_token_id: 2,
    "ffn_hidden_size": 13696,
    "fp32_residual_connection": false,
    "hidden_dropout": 0.0,
    "hidden_size": 4096.
    "kv_channels": 128,
    "layernorm_epsilon": 1e-05,
    "model_type": "chatglm",
    "multi_query_attention": true,
    "multi_query_group_num": 2,
    "num_attention_heads": 32,
    "num_layers": 28,
    "original_rope": true,
    "pad_token_id": 0,
    "padded_vocab_size": 65024,
    "post_layer_norm": true,
    "pre_seq_len": null,
    "prefix_projection": false,
    "quantization_bit": 0,
    "rmsnorm": true,
    "seq_length": 8192,
    "tie_word_embeddings": false,
    "torch_dtype": "bfloat16".
    "transformers_version": "4.41.2",
    "use_cache: true,
    "vocab_size": 65024
}